{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "https://pytorch.org/tutorials/beginner/nn_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is in numpy array format, and has been stored using pickle, a python-specific format for serializing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is 28 x 28, and **is being stored as a flattened row** of length 784:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "example = 43\n",
    "pyplot.imshow(x_train[example].reshape((28, 28)), cmap=\"gray\")\n",
    "print(y_train[example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# convert to torch tensors:\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `TensorDataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor as a list of (x,y) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds) # number of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds[0]) # flat + label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][0].shape # flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][1].shape # label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DataLoader`\n",
    "For **batching**. Very needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bs = 64 # batch size\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `shuffle=True` will shuffle the whole data at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl) # number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check\n",
    "import math\n",
    "math.ceil(len(train_ds) / 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([2, 9, 7, 2, 9, 6, 3, 4, 8, 8, 0, 7, 1, 7, 2, 1, 6, 3, 1, 2, 5, 4, 1, 6,\n",
       "         3, 8, 3, 8, 4, 2, 9, 9, 2, 7, 6, 9, 1, 2, 6, 0, 7, 1, 6, 2, 3, 8, 0, 4,\n",
       "         6, 5, 6, 2, 9, 1, 4, 5, 9, 2, 3, 4, 1, 4, 7, 1])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dl)[0] # first batch\n",
    "# run this cell each time shows a different batch bcz of shuffle=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can loop over `train_dl` like this:\n",
    "```python\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for xb, yb in train_dl: # one batch at a time\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `model.train()`,  `model.eval()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers like **dropout, batchnorm** etc. behave differently on the train and test/val time.\\\n",
    "We need a flag to tell torch when it is train mode or test/val mode.\\\n",
    "`model.train()` sets this flag to `mode=True`, by default.\\\n",
    "`model.eval()` set `mode=False`. Use it during validation/testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # train\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "        \n",
    "    print(epoch, valid_loss / len(valid_dl))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About validation set: can also use `DataLoader` here with bigger batch size, as we are not doing optimization on it.\n",
    "```python\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# don't repeat yourself. Build helpers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sick of the train code? Refactor it:\n",
    "- create a fit **function** that takes model, train, val, epochs, opt, loss as functions\n",
    "- create a fit **method** in your custom modle class (assume that you created a custom model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fit function should be enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    \"\"\"\n",
    "    Helper for fit\n",
    "    Compute loss for a batch using loss_func\n",
    "    \"\"\"\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "                            \n",
    "def fit(model, train_dl, loss_func, opt, epochs=10, valid_dl=None):\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        tr_losses, tr_nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb, opt) for xb, yb in train_dl]\n",
    "            )\n",
    "        train_loss = np.sum(np.multiply(tr_losses, tr_nums)) / np.sum(tr_nums)\n",
    "            \n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        train_history.append(train_loss)\n",
    "        val_history.append(val_loss)\n",
    "        \n",
    "        print(f'epoch: {epoch}, train_loss: {train_loss}, val_loss: {val_loss}')\n",
    "    \n",
    "    history = (train_history, val_history)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a CNN that takes batches of numpy images and return softmax vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    take input size ?, 1, 28, 28\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 3) # input's depth, filters, kernel_size\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 128) \n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        \n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        # If the size is a square you can just specify a single number\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # alternatively:\n",
    "        x = nn.Flatten()(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 26, 26]              60\n",
      "            Conv2d-2           [-1, 16, 11, 11]             880\n",
      "            Linear-3                  [-1, 128]          51,328\n",
      "            Linear-4                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 53,558\n",
      "Trainable params: 53,558\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.20\n",
      "Estimated Total Size (MB): 0.25\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "\n",
    "summary(net, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(net.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = list(train_dl)[0]\n",
    "xb = xb.to(device)\n",
    "yb = yb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3016, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(output, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is really nice that `nn.CrossEntropyLoss` let us pass a softmax vector and target of different shape. Here:\n",
    "- `yb`: Must be class indices, ranging from 0 to C - 1 (C is the number of classes)\\\n",
    "Depends on your dataset, you may have to change your labels to conform to this format.\\\n",
    "Here we don't need to, because yb is both the index and the label (we are classifying number).\n",
    "\n",
    "- For visualization, we may need to convert softmax vectors to label indices, and compute accuracy also. Would be nice if we can show accuracy during each epoch as well. For now it is ok to just print it at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    \"\"\"\n",
    "    out: softmax vectors dim (?, C)\n",
    "    yb: labels dim (?) with yb[i] = 0,1...C-1\n",
    "    \"\"\"\n",
    "    preds = torch.argmax(out, dim=1) # softmax --> labels\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2188, device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(output, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09375\n",
      "0.078125\n",
      "0.109375\n",
      "0.21875\n",
      "0.15625\n",
      "0.0625\n",
      "0.140625\n",
      "0.09375\n",
      "0.15625\n",
      "0.09375\n",
      "0.09375\n",
      "0.109375\n",
      "0.140625\n",
      "0.125\n",
      "0.15625\n",
      "0.1875\n",
      "0.078125\n",
      "0.09375\n",
      "0.109375\n",
      "0.09375\n",
      "0.140625\n",
      "0.171875\n",
      "0.046875\n",
      "0.140625\n",
      "0.109375\n",
      "0.1875\n",
      "0.0625\n",
      "0.140625\n",
      "0.09375\n",
      "0.078125\n",
      "0.078125\n",
      "0.09375\n",
      "0.125\n",
      "0.1875\n",
      "0.109375\n",
      "0.09375\n",
      "0.0625\n",
      "0.140625\n",
      "0.109375\n",
      "0.09375\n",
      "0.09375\n",
      "0.078125\n",
      "0.15625\n",
      "0.171875\n",
      "0.09375\n",
      "0.109375\n",
      "0.15625\n",
      "0.09375\n",
      "0.140625\n",
      "0.140625\n",
      "0.09375\n",
      "0.109375\n",
      "0.09375\n",
      "0.109375\n",
      "0.125\n",
      "0.09375\n",
      "0.078125\n",
      "0.09375\n",
      "0.15625\n",
      "0.171875\n",
      "0.0625\n",
      "0.09375\n",
      "0.109375\n",
      "0.0625\n",
      "0.15625\n",
      "0.171875\n",
      "0.109375\n",
      "0.109375\n",
      "0.125\n",
      "0.125\n",
      "0.078125\n",
      "0.046875\n",
      "0.125\n",
      "0.15625\n",
      "0.0625\n",
      "0.140625\n",
      "0.078125\n",
      "0.15625\n",
      "0.140625\n",
      "0.109375\n",
      "0.140625\n",
      "0.109375\n",
      "0.125\n",
      "0.078125\n",
      "0.203125\n",
      "0.125\n",
      "0.0625\n",
      "0.15625\n",
      "0.09375\n",
      "0.09375\n",
      "0.09375\n",
      "0.171875\n",
      "0.109375\n",
      "0.140625\n",
      "0.09375\n",
      "0.09375\n",
      "0.140625\n",
      "0.09375\n",
      "0.09375\n",
      "0.140625\n",
      "0.125\n",
      "0.078125\n",
      "0.015625\n",
      "0.15625\n",
      "0.140625\n",
      "0.0625\n",
      "0.109375\n",
      "0.078125\n",
      "0.109375\n",
      "0.109375\n",
      "0.125\n",
      "0.15625\n",
      "0.109375\n",
      "0.125\n",
      "0.125\n",
      "0.1875\n",
      "0.140625\n",
      "0.0625\n",
      "0.109375\n",
      "0.15625\n",
      "0.171875\n",
      "0.078125\n",
      "0.109375\n",
      "0.046875\n",
      "0.09375\n",
      "0.1875\n",
      "0.21875\n",
      "0.109375\n",
      "0.09375\n",
      "0.09375\n",
      "0.140625\n",
      "0.09375\n",
      "0.125\n",
      "0.140625\n",
      "0.109375\n",
      "0.125\n",
      "0.109375\n",
      "0.109375\n",
      "0.09375\n",
      "0.109375\n",
      "0.15625\n",
      "0.171875\n",
      "0.09375\n",
      "0.09375\n",
      "0.125\n",
      "0.078125\n",
      "0.125\n",
      "0.09375\n",
      "0.125\n",
      "0.125\n",
      "0.0625\n",
      "0.078125\n",
      "0.15625\n",
      "0.03125\n",
      "0.125\n",
      "0.140625\n",
      "0.109375\n",
      "0.15625\n",
      "0.140625\n",
      "0.203125\n",
      "0.140625\n",
      "0.09375\n",
      "0.171875\n",
      "0.125\n",
      "0.078125\n",
      "0.140625\n",
      "0.078125\n",
      "0.15625\n",
      "0.078125\n",
      "0.078125\n",
      "0.09375\n",
      "0.09375\n",
      "0.125\n",
      "0.109375\n",
      "0.140625\n",
      "0.09375\n",
      "0.0625\n",
      "0.15625\n",
      "0.109375\n",
      "0.203125\n",
      "0.1875\n",
      "0.125\n",
      "0.203125\n",
      "0.078125\n",
      "0.1875\n",
      "0.046875\n",
      "0.078125\n",
      "0.03125\n",
      "0.125\n",
      "0.09375\n",
      "0.046875\n",
      "0.078125\n",
      "0.15625\n",
      "0.078125\n",
      "0.125\n",
      "0.078125\n",
      "0.140625\n",
      "0.078125\n",
      "0.171875\n",
      "0.09375\n",
      "0.03125\n",
      "0.09375\n",
      "0.140625\n",
      "0.15625\n",
      "0.140625\n",
      "0.15625\n",
      "0.046875\n",
      "0.109375\n",
      "0.125\n",
      "0.25\n",
      "0.125\n",
      "0.09375\n",
      "0.046875\n",
      "0.046875\n",
      "0.109375\n",
      "0.09375\n",
      "0.09375\n",
      "0.078125\n",
      "0.09375\n",
      "0.109375\n",
      "0.078125\n",
      "0.109375\n",
      "0.109375\n",
      "0.171875\n",
      "0.125\n",
      "0.125\n",
      "0.09375\n",
      "0.109375\n",
      "0.09375\n",
      "0.09375\n",
      "0.15625\n",
      "0.15625\n",
      "0.125\n",
      "0.109375\n",
      "0.09375\n",
      "0.0625\n",
      "0.125\n",
      "0.109375\n",
      "0.0625\n",
      "0.15625\n",
      "0.125\n",
      "0.140625\n",
      "0.078125\n",
      "0.109375\n",
      "0.109375\n",
      "0.140625\n",
      "0.09375\n",
      "0.078125\n",
      "0.140625\n",
      "0.046875\n",
      "0.046875\n",
      "0.09375\n",
      "0.140625\n",
      "0.125\n",
      "0.109375\n",
      "0.15625\n",
      "0.078125\n",
      "0.140625\n",
      "0.09375\n",
      "0.109375\n",
      "0.078125\n",
      "0.09375\n",
      "0.078125\n",
      "0.078125\n",
      "0.078125\n",
      "0.125\n",
      "0.09375\n",
      "0.109375\n",
      "0.140625\n",
      "0.09375\n",
      "0.125\n",
      "0.09375\n",
      "0.171875\n",
      "0.109375\n",
      "0.09375\n",
      "0.15625\n",
      "0.03125\n",
      "0.09375\n",
      "0.09375\n",
      "0.125\n",
      "0.15625\n",
      "0.140625\n",
      "0.125\n",
      "0.171875\n",
      "0.109375\n",
      "0.171875\n",
      "0.09375\n",
      "0.125\n",
      "0.109375\n",
      "0.15625\n",
      "0.109375\n",
      "0.0625\n",
      "0.09375\n",
      "0.0625\n",
      "0.046875\n",
      "0.09375\n",
      "0.078125\n",
      "0.125\n",
      "0.140625\n",
      "0.109375\n",
      "0.125\n",
      "0.125\n",
      "0.15625\n",
      "0.125\n",
      "0.140625\n",
      "0.078125\n",
      "0.15625\n",
      "0.09375\n",
      "0.140625\n",
      "0.15625\n",
      "0.046875\n",
      "0.15625\n",
      "0.125\n",
      "0.078125\n",
      "0.109375\n",
      "0.109375\n",
      "0.09375\n",
      "0.109375\n",
      "0.15625\n",
      "0.09375\n",
      "0.15625\n",
      "0.046875\n",
      "0.125\n",
      "0.125\n",
      "0.0625\n",
      "0.140625\n",
      "0.09375\n",
      "0.09375\n",
      "0.0625\n",
      "0.09375\n",
      "0.046875\n",
      "0.09375\n",
      "0.109375\n",
      "0.09375\n",
      "0.109375\n",
      "0.046875\n",
      "0.109375\n",
      "0.0625\n",
      "0.15625\n",
      "0.09375\n",
      "0.109375\n",
      "0.109375\n",
      "0.0625\n",
      "0.15625\n",
      "0.078125\n",
      "0.09375\n",
      "0.171875\n",
      "0.125\n",
      "0.171875\n",
      "0.046875\n",
      "0.09375\n",
      "0.109375\n",
      "0.09375\n",
      "0.0625\n",
      "0.0625\n",
      "0.09375\n",
      "0.109375\n",
      "0.125\n",
      "0.09375\n",
      "0.046875\n",
      "0.140625\n",
      "0.046875\n",
      "0.109375\n",
      "0.078125\n",
      "0.15625\n",
      "0.125\n",
      "0.046875\n",
      "0.078125\n",
      "0.078125\n",
      "0.140625\n",
      "0.140625\n",
      "0.140625\n",
      "0.203125\n",
      "0.15625\n",
      "0.140625\n",
      "0.0625\n",
      "0.078125\n",
      "0.15625\n",
      "0.09375\n",
      "0.09375\n",
      "0.125\n",
      "0.109375\n",
      "0.0625\n",
      "0.140625\n",
      "0.078125\n",
      "0.078125\n",
      "0.125\n",
      "0.140625\n",
      "0.078125\n",
      "0.125\n",
      "0.03125\n",
      "0.15625\n",
      "0.09375\n",
      "0.15625\n",
      "0.109375\n",
      "0.140625\n",
      "0.140625\n",
      "0.0625\n",
      "0.125\n",
      "0.140625\n",
      "0.125\n",
      "0.125\n",
      "0.171875\n",
      "0.125\n",
      "0.109375\n",
      "0.078125\n",
      "0.1875\n",
      "0.125\n",
      "0.140625\n",
      "0.171875\n",
      "0.125\n",
      "0.15625\n",
      "0.15625\n",
      "0.109375\n",
      "0.171875\n",
      "0.09375\n",
      "0.09375\n",
      "0.046875\n",
      "0.171875\n",
      "0.203125\n",
      "0.140625\n",
      "0.078125\n",
      "0.0625\n",
      "0.0625\n",
      "0.0625\n",
      "0.125\n",
      "0.0625\n",
      "0.15625\n",
      "0.09375\n",
      "0.125\n",
      "0.109375\n",
      "0.109375\n",
      "0.125\n",
      "0.1875\n",
      "0.125\n",
      "0.171875\n",
      "0.140625\n",
      "0.09375\n",
      "0.046875\n",
      "0.109375\n",
      "0.0625\n",
      "0.0625\n",
      "0.140625\n",
      "0.109375\n",
      "0.09375\n",
      "0.09375\n",
      "0.125\n",
      "0.0625\n",
      "0.140625\n",
      "0.0625\n",
      "0.09375\n",
      "0.046875\n",
      "0.078125\n",
      "0.09375\n",
      "0.03125\n",
      "0.09375\n",
      "0.09375\n",
      "0.078125\n",
      "0.09375\n",
      "0.109375\n",
      "0.125\n",
      "0.140625\n",
      "0.125\n",
      "0.140625\n",
      "0.15625\n",
      "0.078125\n",
      "0.171875\n",
      "0.109375\n",
      "0.09375\n",
      "0.140625\n",
      "0.09375\n",
      "0.125\n",
      "0.109375\n",
      "0.109375\n",
      "0.078125\n",
      "0.125\n",
      "0.125\n",
      "0.078125\n",
      "0.125\n",
      "0.09375\n",
      "0.09375\n",
      "0.171875\n",
      "0.109375\n",
      "0.125\n",
      "0.109375\n",
      "0.09375\n",
      "0.09375\n",
      "0.109375\n",
      "0.140625\n",
      "0.078125\n",
      "0.15625\n",
      "0.109375\n",
      "0.140625\n",
      "0.109375\n",
      "0.03125\n",
      "0.109375\n",
      "0.09375\n",
      "0.09375\n",
      "0.125\n",
      "0.0625\n",
      "0.125\n",
      "0.09375\n",
      "0.171875\n",
      "0.140625\n",
      "0.09375\n",
      "0.125\n",
      "0.09375\n",
      "0.15625\n",
      "0.09375\n",
      "0.140625\n",
      "0.09375\n",
      "0.109375\n",
      "0.109375\n",
      "0.140625\n",
      "0.078125\n",
      "0.109375\n",
      "0.140625\n",
      "0.09375\n",
      "0.078125\n",
      "0.125\n",
      "0.078125\n",
      "0.109375\n",
      "0.203125\n",
      "0.140625\n",
      "0.109375\n",
      "0.140625\n",
      "0.15625\n",
      "0.125\n",
      "0.15625\n",
      "0.078125\n",
      "0.15625\n",
      "0.15625\n",
      "0.125\n",
      "0.109375\n",
      "0.09375\n",
      "0.03125\n",
      "0.125\n",
      "0.09375\n",
      "0.078125\n",
      "0.09375\n",
      "0.140625\n",
      "0.046875\n",
      "0.125\n",
      "0.140625\n",
      "0.046875\n",
      "0.078125\n",
      "0.109375\n",
      "0.09375\n",
      "0.09375\n",
      "0.0625\n",
      "0.125\n",
      "0.125\n",
      "0.078125\n",
      "0.109375\n",
      "0.078125\n",
      "0.078125\n",
      "0.140625\n",
      "0.109375\n",
      "0.171875\n",
      "0.078125\n",
      "0.109375\n",
      "0.046875\n",
      "0.15625\n",
      "0.0625\n",
      "0.125\n",
      "0.078125\n",
      "0.140625\n",
      "0.15625\n",
      "0.109375\n",
      "0.09375\n",
      "0.078125\n",
      "0.125\n",
      "0.09375\n",
      "0.125\n",
      "0.1875\n",
      "0.0625\n",
      "0.171875\n",
      "0.0625\n",
      "0.09375\n",
      "0.1875\n",
      "0.15625\n",
      "0.109375\n",
      "0.125\n",
      "0.140625\n",
      "0.140625\n",
      "0.125\n",
      "0.09375\n",
      "0.078125\n",
      "0.234375\n",
      "0.09375\n",
      "0.109375\n",
      "0.109375\n",
      "0.046875\n",
      "0.125\n",
      "0.046875\n",
      "0.109375\n",
      "0.140625\n",
      "0.09375\n",
      "0.0625\n",
      "0.078125\n",
      "0.09375\n",
      "0.03125\n",
      "0.125\n",
      "0.109375\n",
      "0.03125\n",
      "0.0625\n",
      "0.109375\n",
      "0.09375\n",
      "0.09375\n",
      "0.171875\n",
      "0.078125\n",
      "0.078125\n",
      "0.078125\n",
      "0.109375\n",
      "0.171875\n",
      "0.09375\n",
      "0.1875\n",
      "0.125\n",
      "0.109375\n",
      "0.140625\n",
      "0.09375\n",
      "0.015625\n",
      "0.140625\n",
      "0.125\n",
      "0.03125\n",
      "0.0625\n",
      "0.125\n",
      "0.09375\n",
      "0.109375\n",
      "0.109375\n",
      "0.125\n",
      "0.046875\n",
      "0.109375\n",
      "0.0625\n",
      "0.109375\n",
      "0.078125\n",
      "0.140625\n",
      "0.09375\n",
      "0.15625\n",
      "0.0625\n",
      "0.171875\n",
      "0.15625\n",
      "0.1875\n",
      "0.078125\n",
      "0.171875\n",
      "0.109375\n",
      "0.125\n",
      "0.09375\n",
      "0.171875\n",
      "0.140625\n",
      "0.0625\n",
      "0.140625\n",
      "0.1875\n",
      "0.140625\n",
      "0.046875\n",
      "0.171875\n",
      "0.09375\n",
      "0.09375\n",
      "0.140625\n",
      "0.15625\n",
      "0.046875\n",
      "0.078125\n",
      "0.03125\n",
      "0.125\n",
      "0.125\n",
      "0.09375\n",
      "0.171875\n",
      "0.125\n",
      "0.140625\n",
      "0.109375\n",
      "0.171875\n",
      "0.03125\n",
      "0.140625\n",
      "0.125\n",
      "0.0625\n",
      "0.109375\n",
      "0.078125\n",
      "0.140625\n",
      "0.140625\n",
      "0.171875\n",
      "0.109375\n",
      "0.078125\n",
      "0.140625\n",
      "0.109375\n",
      "0.078125\n",
      "0.1875\n",
      "0.09375\n",
      "0.140625\n",
      "0.03125\n",
      "0.140625\n",
      "0.140625\n",
      "0.078125\n",
      "0.09375\n",
      "0.203125\n",
      "0.0625\n",
      "0.078125\n",
      "0.078125\n",
      "0.109375\n",
      "0.0625\n",
      "0.109375\n",
      "0.140625\n",
      "0.09375\n",
      "0.078125\n",
      "0.046875\n",
      "0.09375\n",
      "0.078125\n",
      "0.140625\n",
      "0.15625\n",
      "0.125\n",
      "0.078125\n",
      "0.078125\n",
      "0.09375\n",
      "0.125\n",
      "0.125\n",
      "0.125\n",
      "0.0625\n",
      "0.078125\n",
      "0.09375\n",
      "0.09375\n",
      "0.0625\n",
      "0.09375\n",
      "0.109375\n",
      "0.109375\n",
      "0.125\n",
      "0.140625\n",
      "0.078125\n",
      "0.078125\n",
      "0.078125\n",
      "0.203125\n",
      "0.078125\n",
      "0.125\n",
      "0.0625\n",
      "0.140625\n",
      "0.09375\n",
      "0.140625\n",
      "0.125\n",
      "0.09375\n",
      "0.0625\n",
      "0.125\n",
      "0.140625\n",
      "0.0625\n",
      "0.109375\n",
      "0.09375\n",
      "0.15625\n",
      "0.09375\n",
      "0.078125\n",
      "0.0625\n",
      "0.125\n",
      "0.09375\n",
      "0.109375\n",
      "0.109375\n",
      "0.125\n",
      "0.0625\n",
      "0.078125\n",
      "0.125\n",
      "0.0625\n",
      "0.0625\n",
      "0.078125\n",
      "0.140625\n",
      "0.09375\n",
      "0.15625\n",
      "0.03125\n",
      "0.125\n",
      "0.109375\n",
      "0.078125\n",
      "0.109375\n",
      "0.125\n",
      "0.109375\n",
      "0.140625\n",
      "0.140625\n",
      "0.1875\n",
      "0.125\n",
      "0.078125\n",
      "0.03125\n",
      "0.109375\n",
      "0.078125\n",
      "0.078125\n",
      "0.078125\n",
      "0.140625\n",
      "0.140625\n",
      "0.03125\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "# Show accuracy for all batches\n",
    "# expected to increase after training\n",
    "batches = iter(train_dl)\n",
    "\n",
    "for batch in batches:\n",
    "    \n",
    "    xb, yb = batch\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    \n",
    "    print(accuracy(net(xb), yb).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to load ALL the data into `device`. When and where to do it? \\\n",
    "Can we do it in the training loop? Sure we can.\\\n",
    "Can we take a batch out of GPU after training on it, we could. Should we? I dunno?\\\n",
    "Can we do it in our lambda function? Yes we can, but that only works for input. When we compute the loss, accuracy, both `yb` and `output` must be in the same `device`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belows, we put all of them in device from the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders to gpu\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n",
    "\n",
    "# to gpu\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 1.6787176359176637, val_loss: 1.5895173076629638\n",
      "epoch: 1, train_loss: 1.5182633846282958, val_loss: 1.4957787641525269\n",
      "epoch: 2, train_loss: 1.4968842280578614, val_loss: 1.49022708568573\n",
      "epoch: 3, train_loss: 1.4910934843444825, val_loss: 1.4879058990478515\n",
      "epoch: 4, train_loss: 1.486659225463867, val_loss: 1.4832769107818604\n",
      "epoch: 5, train_loss: 1.4830427059173583, val_loss: 1.4809241144180298\n",
      "epoch: 6, train_loss: 1.4808132033920287, val_loss: 1.482196203994751\n",
      "epoch: 7, train_loss: 1.4796199643707275, val_loss: 1.48014649848938\n",
      "epoch: 8, train_loss: 1.478877621383667, val_loss: 1.4792995969772338\n",
      "epoch: 9, train_loss: 1.476932453842163, val_loss: 1.477655746459961\n",
      "epoch: 10, train_loss: 1.475439107131958, val_loss: 1.481100264930725\n",
      "epoch: 11, train_loss: 1.4743390644073486, val_loss: 1.4776422695159912\n",
      "epoch: 12, train_loss: 1.4738734788513184, val_loss: 1.4777868888854981\n",
      "epoch: 13, train_loss: 1.4720510172271728, val_loss: 1.476436173439026\n",
      "epoch: 14, train_loss: 1.4718342863464355, val_loss: 1.4750985431671142\n",
      "epoch: 15, train_loss: 1.4708773385620117, val_loss: 1.4762566287994385\n",
      "epoch: 16, train_loss: 1.4706946411132813, val_loss: 1.4763980159759522\n",
      "epoch: 17, train_loss: 1.4697231665802002, val_loss: 1.4754959857940675\n",
      "epoch: 18, train_loss: 1.4694959489440917, val_loss: 1.4741986377716065\n",
      "epoch: 19, train_loss: 1.4687773949432372, val_loss: 1.4748471881866454\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "model = net\n",
    "epochs = 20\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "history = fit(model=model,\n",
    "    epochs=epochs, \n",
    "    loss_func=loss_func, opt=opt,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9939058503836317\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for all batches should improve now\n",
    "batches = iter(train_dl) # remember not to re-use generators :>\n",
    "acc = 0\n",
    "count = 0\n",
    "\n",
    "for batch in batches:\n",
    "    count += len(batch)\n",
    "    xb, yb = batch\n",
    "    acc += (accuracy(net(xb), yb).item()) * len(batch)\n",
    "print('accuracy =', acc / count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(history[0])\n",
    "pyplot.plot(history[1])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap:\n",
    "- Use `Dataset` --> `DataLoader` for batching\n",
    "- helper functions: fit, accuracy can be re-used for other problems as well\n",
    "- model.train() and model.eval() to make sure batchnorm and dropout works correctly.\n",
    "- add data to gpu should be performed for all train-dev-test.\n",
    "- be aware of format for parameters in `nn.CrossEntropyLoss()`\n",
    "\n",
    "Problems not addressed:\n",
    "- What if images come in different shape? Assuming that we need to massage them before the forward pass, where we should do it? Using a Lambda layer or `preprocess` function is both ok I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpy",
   "language": "python",
   "name": "cvpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
